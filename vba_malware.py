import os
import sys
import logging
import re
import datetime
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import joblib
import glob
import hashlib
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
import time
if sys.version_info >= (3, 0):
    from oletools.olevba3 import VBA_Parser
else:
    from oletools.olevba import VBA_Parser


# Config Notes:
# Data Save config:
save_directory = "saves"
# Features are saved as a dataframe
# Models are saved as a dict containing the features (dataframe from above) and a trained model
process_files_from_small_folders_flag = False
process_files_from_big_folders_flag = False

save_vba_flag = False
load_last_feature_flag = True
save_features_flag = False

Grid_Search_Flag = False

train_model_flag = False
train_with_all_samples_flag = True
save_model_flag = False
load_last_model_flag = True

predict_files_flag = False


# VT Searches:
## p:20+ entity:file tag:macros size:10MB- fs:2016-01-01+ fs:2023-05-01-
## Did not do well: p:1- entity:file tag:macros size:10MB- fs:2016-01-01+ fs:2023-05-01-
##     Slightly better: fs:2016-01-01+ fs:2023-05-01- type:document tag:macros size:10MB- p:1-
# for benign look past a year to make sure AV's have had time to write signatures
malware_folder = "samples_malicious"
malware_folder_big = "D:\\Malware\\samples_malicious"
benign_folder = "samples_benign"
benign_folder_big = "D:\\Malware\\samples_benign"
benign_folder_big = "D:\\Malware\\samples_benign_p1_doc"
vocab_path = "vba_vocab.txt"
image_path = "figures"
save_vba_folder = "output_vba"
predict_files_folder = "samples_for_chart"
predict_files_folder = "samples_for_chart_benign"

# Model Config
number_of_trees = 100

# Create output Folders if they don't already exist
if save_model_flag and not os.path.exists(save_directory):
    os.makedirs(save_directory)
if save_vba_flag and not os.path.exists(save_vba_folder):
    os.makedirs(save_vba_folder)

# Globals (I know, I'm bad)
df_full = pd.DataFrame()
count_of_added_mal_samples = 0
count_of_added_mal_samples_failed = 0
count_of_added_benign_samples = 0
count_of_added_benign_samples_failed = 0
global_failed_sample_list = []



# level=logging.INFO
# level=logging.ERROR
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
logging.info("Starting")

logging.info("Loading Vocab List")
if not os.path.exists(vocab_path):
    logging.error(f"Vocab file missing: {vocab_path}")
    exit()
with open(vocab_path) as vocabfile:
    lines = vocabfile.readlines()
    lines = [x.strip().lower() for x in lines]
vocab_list = list(set(lines))
if " " in vocab_list:
    vocab_list.remove(" ")
if "" in vocab_list:
    vocab_list.remove("")

def generate_log_filename(prependString = ""):
    # Current time formatted as 'YYYY-MM-DD_HH-MM-SS'
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    # Generate filename
    filename = f"{prependString}_{timestamp}.gz"
    return filename

def return_decoded_value(value):
    if type(value) is bytes:
        value = value.decode('utf-8')
    elif type(value) is not str:
        value = value.decode("ascii", "ignore")
    else:
        value = value

    return value.strip('\r\n')

def get_vba( myfile, source='filepath'):
    """
    Given a file, parses out the stream paths, vba code, and vba filenames for each.
    :param myfile: filename or data
    :param source: type of data being passed in.  Either "filepath" to indicate we need to read from disk or
    "filecontents" (or anything else) meaning that the file contents are being passed as a parameter.
    :return: Dict {'extracted_vba': allcode, 'stream_path': pathnames, 'filename_vba': filenames}
    """
    if source == 'filepath':
        filedata = open(myfile, 'rb').read()
    else:
        filedata = myfile

    try:
        vbaparser = VBA_Parser('doesnotexist-passed by filedata param', data=filedata)
        pathnames = ''
        if vbaparser.detect_vba_macros():
            filenameslist = []
            pathnameslist = []
            vbacodelist = []
            for (filename, stream_path, filename_vba, extracted_vba) in vbaparser.extract_macros():
                vbacodelist.append(return_decoded_value(extracted_vba))
                if not pathnames:
                    pathnameslist.append(return_decoded_value(stream_path))
                    filenameslist.append(return_decoded_value(filename_vba))
                else:
                    pathnameslist.append(return_decoded_value(stream_path))
                    filenameslist.append(return_decoded_value(filename_vba))
            allcode = "\n\n\n\n".join(vbacodelist)
            filenames = ", ".join(filenameslist)
            pathnames = ", ".join(pathnameslist)

        else:
            pathnames = 'No VBA Macros found'
            filenames = 'No VBA Macros found'
            allcode = 'No VBA Macros found'

    except Exception as e:
        pathnames = 'Error:' + str(e)
        filenames = 'Error:' + str(e)
        allcode = 'Error:' + str(e)
        logging.error(f"Error in filenames/code {myfile} {str(e)}")

    if save_vba_flag:
        # TODO: Try/Except like other file write operations. maybe check to see if file already exists
        # coming to us with the pre-pended malware_folder so we have to remove that to get the
        # This is slow, if I need to speed it up I can use the job lib
        original_file_name = os.path.basename(myfile)
        output_path = os.path.join(save_vba_folder, original_file_name)
        logging.info(f"Saving VBA output to {output_path}")
        open(output_path, "w").write(allcode)

    return {'extracted_vba': allcode, 'stream_path': pathnames, 'filename_vba': filenames}

def get_entropy( vbcodeSeries):
    """
    Helper function to return entropy calculation value
    :param vbcodeSeries: pandas series of values
    :return: entropy of the set of values.
    """
    probs = vbcodeSeries.value_counts() / len(vbcodeSeries)
    entropy = stats.entropy(probs)
    return entropy

def get_language_features(vba_source):
    features = {}

    # How many interesting/reserved language words are in the VBA?
    vectorizerizer = CountVectorizer(vocabulary=vocab_list,
                                         lowercase=True,
                                         decode_error='ignore',
                                         token_pattern=r"(?u)\b\w[\w\.]+\b")

    # This is probably a really inefficient way to do this
    extacted_vba = [vba_source] # turn it into a list because the CountVectorizer expects a list
    word_count_matrix = vectorizerizer.fit_transform(extacted_vba)
    word_count_array = word_count_matrix.toarray()
    feature_names = vectorizerizer.get_feature_names_out()
    aggregate_counts = word_count_array.sum(axis=0)
    frequency_dict = dict(zip("count_" + feature_names, aggregate_counts))

    # Add to our main dictionary of features
    features.update(frequency_dict)
    #for word in vectorizerizer.get_feature_names_out():
    #    features["count_" + word] = vectorizerizer.vocabulary_.get(word)


    # TF-IDF Transformer
    # Interesting word frequency per document (VBA)
    tf_transformer = TfidfTransformer()
    tfidf_matrix = tf_transformer.fit_transform(word_count_matrix)
    tfidf_array = tfidf_matrix.toarray()
    aggregated_tfidf = tfidf_array.sum(axis=0)
    tfidf_dict = dict(zip("tfidf_" + feature_names, aggregated_tfidf))
    # Add to our main dictionary of features
    features.update(tfidf_dict)

    #word_freq_matrix = model_tfidf_trans.fit_transform(word_count_matrix.toarray())
    #for word in model_tfidf_trans.get_feature_names_out():
    #    features["tfidf_" + word] = model_tfidf_trans.vocabulary_.get(word)

    # Get all other language features
    features.update(get_vba_features(vba_source))
    return features

def load_model():
    try:
        dat_files = glob.glob(os.path.join(save_directory, '*model*.gz'))
        latest_file = max(dat_files, key=os.path.getctime, default=None)

        if latest_file is not None:
            print("Loading Model from " + latest_file)
            logging.info("Loading Model from " + latest_file)
            saved_data = joblib.load(latest_file)
            return saved_data

        if latest_file is None:
            message = "Error loading models from disk: No latest file found"
            logging.error(message)
            raise IOError(message)

    except TypeError as y:
        message = "Pickle file may be corrupted, please verify you have a proper pickle file {}".format(str(y))
        logging.error(message)
        raise IOError(message)

    except Exception as e:
        message = "Error loading models from disk: {}".format(str(e))
        logging.error(message)
        raise IOError(message)

def load_features():
    try:
        dat_files = glob.glob(os.path.join(save_directory, '*feature*.gz'))
        latest_file = max(dat_files, key=os.path.getctime, default=None)
        if latest_file is not None:
            print("Loading Features from " + latest_file)
            logging.info("Loading Features from " + latest_file)
            df_features = joblib.load(latest_file)
            return df_features
        if latest_file is None:
            message = "Error loading features from disk: No latest file found"
            logging.error(message)
            raise IOError(message)

    except TypeError as y:
        message = "Pickle file may be corrupted, please verify you have a proper pickle file {}".format(str(y))
        logging.error(message)
        raise IOError(message)

    except Exception as e:
        message = "Error loading features from disk: {}".format(str(e))
        logging.error(message)
        raise IOError(message)

def save_features(features):
    try:
        savefile = generate_log_filename("features")
        savefile = os.path.join(save_directory, savefile)
        logging.info("Saving Features " + savefile)
        joblib.dump(features, savefile, compress="gzip")
    except Exception as e:
        message = "Error saving model data to disk: {}".format(str(e))
        logging.error(message)
def save_model(model):
    global number_of_trees
    try:
        savefile = generate_log_filename(f"model_{number_of_trees}_trees")
        savefile = os.path.join(save_directory, savefile)
        logging.info(f"Saving Model to {savefile} " )
        print(f"Saving Model to {savefile} ")
        joblib.dump(model, savefile, compress="gzip")
    except Exception as e:
        message = "Error saving model data to disk: {}".format(str(e))
        logging.error(message)

def get_vba_features(vb):
    """
    Given VB code as a string input, returns various summary data about it.
    :param vb: vbacode as one large multiline string
    :return: pandas Series that can be used in concert with the pandas DataFrame apply method
    """
    allfunctions = []
    all_num_functions = []
    all_locs = []
    entropy_func_names = 0
    avg_param_per_func = 0.0
    functions_str = ''
    vba_cnt_func_loc_ratio = 0.0
    vba_cnt_comment_loc_ratio = 0.0

    if vb == 'No VBA Macros found' or vb[0:6] == 'Error:':
        functions = 'None'
        num_functions = 0
        loc = 0
        avg_loc_func = 0
        num_comments = 0
        entropy_chars = 0
        entropy_words = 0
    else:
        functions = {}
        num_comments = vb.count("'")  # TODO: do a better job
        lines = vb.splitlines()
        new_lines = []
        num_functions = 0
        entropy_chars = get_entropy(pd.Series(vb.split(' ')))
        entropy_words = get_entropy(pd.Series(list(vb)))
        reFunction = re.compile(r'.*\s?[Sub|Function]\s+([a-zA-Z0-9_]+)\((.*)\)')
        for line in lines:
            if len(line.strip()) > 0:
                new_lines.append(line)

            function_name_matches = reFunction.findall(line)
            num_params = 0
            if len(function_name_matches) > 0:
                num_functions = num_functions + 1
                num_params = function_name_matches[0][1].count(',') + 1
                if len(function_name_matches[0][1].strip()) <= 0:
                    num_params = 0
                functions[function_name_matches[0][0]] = num_params

        loc = len(new_lines)
        if len(functions) > 0:
            function_name_str = ''.join(functions.keys())
            entropy_func_names = get_entropy(pd.Series(list(function_name_str)))
            functions_str = ', '.join(functions.keys())
            param_list = functions.values()
            avg_param_per_func = (1.0 * sum(param_list)) / len(param_list)
        if loc > 0:
            vba_cnt_func_loc_ratio = (1.0 * len(functions)) / loc
            vba_cnt_comment_loc_ratio = (1.0 * num_comments) / loc
        if num_functions <= 0:
            avg_loc_func = float(loc)
        else:
            avg_loc_func = float(loc) / num_functions

    return {'function_names': functions_str,
                      'vba_avg_param_per_func': avg_param_per_func,
                      'vba_cnt_comments': num_comments,
                      'vba_cnt_functions': num_functions,
                      'vba_cnt_loc': loc,
                      'vba_cnt_func_loc_ratio': vba_cnt_func_loc_ratio,
                      'vba_cnt_comment_loc_ratio': vba_cnt_comment_loc_ratio,
                      'vba_entropy_chars': entropy_chars,
                      'vba_entropy_words': entropy_words,
                      'vba_entropy_func_names': entropy_func_names,
                      'vba_mean_loc_per_func': avg_loc_func
                      }

def get_features_from_files_in_path(folderPath, label):
    '''
    :param folderPath: Path to the folder containing the files
    :param label: "malicious" or "benign"
    :return: Df containing all the data
    '''
    global  df_full
    logging.info(f"Loading {label} Samples from {folderPath}")
    if label == "malicious" or label == "benign" or label == "predict":
        pass
    else:
        logging.error(f"Label does not have proper string: {label}")
        exit()

    # Making a list of dict's because growing a dataframe row-by-row is (apparently) terrible
    list_of_dict_features = []
    failed_sample_list = []
    if not os.path.exists(folderPath):
        logging.error(f"Can not load samples - folder does not exist: {folderPath} ")
        return pd.DataFrame()
    for file in os.listdir(folderPath):
        full_path = os.path.join(folderPath, file)

        # Check to see if we already have processed this file.
        file_hash = getFileNameHash(full_path)
        exists = False
        if len(df_full.index) != 0:
            if 'file_hash' in df_full and 'label' in df_full:   # TODO: this should never happen, but for some reason it's popping up that df_full['file_hash'] doesn't exist
                exists = ((df_full['label'] == label) & (df_full['file_hash'] == file_hash)).any()
            else:
                logging.warning("file_hash column is not in df_full, so we can't verify this file isn't already in the dataset")
        if exists:
            logging.info(f"Skipping already processed: {full_path}")
            continue
        logging.info("Loading: " + full_path)
        dict_vba_code = get_vba(myfile = full_path)
        if 'No VBA Macros found' in dict_vba_code['extracted_vba']:
            logging.info(f"Skipping because no VBA found in {full_path}" )
            failed_sample_list.append(full_path)
        else:
            dict_features = get_language_features(dict_vba_code['extracted_vba'])
            dict_features['file_hash'] = file_hash
            dict_features.update(dict_vba_code)
            dict_features['label'] = label
            list_of_dict_features.append(dict_features)

    count_of_added_samples = len(list_of_dict_features)
    count_of_failed_samples = len(failed_sample_list)
    logging.info(f"Samples ({label}) processed: {count_of_added_samples}")
    logging.info(f"Samples ({label}) failed to be processed: {count_of_failed_samples}")

    # update global counts
    global count_of_added_mal_samples, count_of_added_mal_samples_failed, count_of_added_benign_samples, count_of_added_benign_samples_failed, global_failed_sample_list
    if label == "malicious":
        count_of_added_mal_samples += count_of_added_samples
        count_of_added_mal_samples_failed += count_of_failed_samples
        logging.info(f"Loaded {count_of_added_mal_samples} malicious samples")
    elif label == "benign":
        count_of_added_benign_samples += count_of_added_samples
        count_of_added_benign_samples_failed += count_of_failed_samples
        logging.info(f"Loaded {count_of_added_benign_samples} benign samples")
    elif label == "predict":
        logging.info(f"Predicted Sample. ")
    else:
        logging.error(f"Label does not have proper string: {label}")
        exit()

    global_failed_sample_list = failed_sample_list + global_failed_sample_list
    # Combine all the data
    df_ret = pd.DataFrame(list_of_dict_features)
    return df_ret

def getFileNameHash(full_path):
    # assume if the file is named with sha256, that it's correct, else this will slow down analysis
    filenameToHash = os.path.basename(full_path)
    pattern = r'^[a-fA-F0-9]{64}$'
    match = re.match(pattern, filenameToHash)
    if match is not None:
        return filenameToHash.lower()
    else:
        logging.info(f"Hashing {full_path}")
        #print(f"Hashing {full_path}")
        sha256_hash = hashlib.sha256()
        try:
            with open(full_path, "rb") as sampleToHash:
                # Read and update hash string value in blocks of 4K
                for byte_block in iter(lambda: sampleToHash.read(4096), b""):
                    sha256_hash.update(byte_block)
        except FileNotFoundError:
            logging.error(f"Failed to hash file because it was not found: {full_path}")
            exit()
        except Exception as e:
            logging.error(f"Failed to hash file: {str(e)}")
            exit()

    logging.info(f"Hashed to {sha256_hash.hexdigest().lower()}")
    return sha256_hash.hexdigest().lower()



####################################### Main #######################################
if load_last_feature_flag:
    if load_last_feature_flag:
        logging.info("Loading Features from file")
        df_loaded = load_features()
        if 'file_hash' not in df_loaded:
            logging.warning("Loaded Features does not contain file_hash (it should)")
        if 'label' not in df_loaded:
            logging.warning("Loaded Features does not contain label (it should)")
        logging.info(f"Loaded {len(df_loaded.index)} rows from file")
        df_full = pd.concat([df_full, df_loaded], ignore_index=True, sort=False)

if process_files_from_small_folders_flag:
    start_time = time.time()
    df_mal = get_features_from_files_in_path(malware_folder, "malicious")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Loading {malware_folder} completed. It took {elapsed_time:.2f} seconds.")
    df_full = pd.concat([df_full, df_mal], ignore_index=True, sort=False)

    start_time = time.time()
    df_benign = get_features_from_files_in_path(benign_folder,"benign")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Loading {benign_folder} completed. It took {elapsed_time:.2f} seconds.")
    df_full = pd.concat([df_full, df_benign], ignore_index=True, sort=False)


    logging.info(f"New Malicious Samples: {count_of_added_mal_samples}")
    logging.info(f"New Benign Samples: {count_of_added_benign_samples}")
    print(f"New Malicious Samples: {count_of_added_mal_samples}")
    print(f"New Benign Samples: {count_of_added_benign_samples}")



if process_files_from_big_folders_flag:
    start_time = time.time()
    df_mal = get_features_from_files_in_path(malware_folder_big, "malicious")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Loading {malware_folder_big} completed. It took {elapsed_time:.2f} seconds.")
    df_full = pd.concat([df_full, df_mal], ignore_index=True, sort=False)

    start_time = time.time()
    df_benign = get_features_from_files_in_path(benign_folder_big, "benign")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Loading {benign_folder_big} completed. It took {elapsed_time:.2f} seconds.")
    df_full = pd.concat([df_full, df_benign], ignore_index=True, sort=False)

    logging.info(f"New Malicious Samples: {count_of_added_mal_samples}")
    logging.info(f"New Benign Samples: {count_of_added_benign_samples}")
    print(f"New Malicious Samples: {count_of_added_mal_samples}")
    print(f"New Benign Samples: {count_of_added_benign_samples}")

if save_features_flag:
    save_features(df_full)

if load_last_model_flag:
    forest = load_model()

if df_full is None:
    logging.error("Error Loading features. Exiting")
    exit()

if train_model_flag:
    if load_last_model_flag:
        logging.warning("Flags set for Training a Model AND loading a model from disk")
    logging.info(f"Preparing data to train on {len(df_full.index)} rows ")
    # Remove anything I don't want and train with the DataFrame that we loaded from files one by one or from model backup
    # This is ineffient - if I need more speed, do each of these all at once
    start_time = time.time()

    if "function_names" in df_full:
        logging.info("Dropping Function names from features list")
        df_full.drop(columns=["function_names"], inplace=True)
    if "extracted_vba" in df_full:
        logging.info("Dropping extracted_vba from features list")
        df_full.drop(columns=["extracted_vba"], inplace=True)
    if "filename_vba" in df_full:
        logging.info("Dropping filename_vba from features list")
        df_full.drop(columns=["filename_vba"], inplace=True)
    if "stream_path" in df_full:
        logging.info("Dropping stream_path from features list")
        df_full.drop(columns=["stream_path"], inplace=True)
    if "file_hash" in df_full:
        logging.info("Dropping file hash from features list")
        df_full.drop(columns=["file_hash"], inplace=True)

    end_time = time.time()
    elapsed_time = end_time - start_time
    # print(f"Dropping unwanted columns took {elapsed_time:.2f} seconds.") - only 7 or 8 seconds with a 160k rows

    # Data Slicing
    X = df_full.values[:, : -1]  # removing the last column's (the feature we want to predict which is 'malicious' or 'benign')
    Y = df_full.values[:, -1]
    if len(np.unique(Y)) != 2:
        logging.error("The dataset does not have two labels: " + str(np.unique(Y)))
        exit()

    forest = RandomForestClassifier(n_estimators=number_of_trees, max_features=.2, n_jobs=-1)  # TODO: Play with these hyper parameters
    logging.info("Training...")
    # ToDo: Look into LabelBinarizer()
    #lb = LabelBinarizer()
    #y_train = np.array([number[0] for number in lb.fit_transform(y_train)])
    start_time = time.time()
    if train_with_all_samples_flag:
        forest.fit(X, Y)
    else:
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,
                                                            random_state=100)  # ToDo: play with the test values
        forest.fit(X_train, y_train)
        y_pred = forest.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy:", accuracy)  # 0.9969434221196432

        # ToDo: test these
        # recall = cross_val_score(forest, X_train, y_train, cv=5, scoring='recall')
        # precision = cross_val_score(forest, X_train, y_train, cv=5, scoring='precision')
        # accuracy = cross_val_score(forest, X_train, y_train, cv=5, scoring='accuracy')
        # f1_score = cross_val_score(forest, X_train, y_train, cv=5, scoring='f1_macro')
        # print(f"Recall {recall}")
        # print(f"Precision {precision}")
        # print(f"Accuracy {accuracy}")
        # print(f"F1 Score {f1_score}")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Training completed. It took {elapsed_time:.2f} seconds for {number_of_trees} decision trees.")

if Grid_Search_Flag:
    forest = RandomForestClassifier( max_features=.2, n_jobs=-1)
    X = df_full.values[:,: -1]  # removing the last column's (the feature we want to predict which is 'malicious' or 'benign')
    Y = df_full.values[:, -1]
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,
                                                        random_state=100)
    param_grid = {
        'n_estimators': [10, 50, 100, 200, 500]
    }
    grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    # Best number of trees
    best_n_estimators = grid_search.best_params_['n_estimators']

    # Print the best number of trees and the corresponding accuracy
    print("Best number of trees:", best_n_estimators)
    print("Best cross-validation score (accuracy):", grid_search.best_score_)

    # Optionally, evaluate on the test set
    best_rf = grid_search.best_estimator_
    test_accuracy = best_rf.score(X_test, y_test)
    print("Test set accuracy:", test_accuracy)

    # Best number of trees: 500
    # Best cross-validation score (accuracy): 0.9952775077800146
    # Test set accuracy: 0.9957587475831098

def check_dataframes(df: pd.DataFrame, df_sus: pd.DataFrame):
    print("First Dataframe colm count: " + str(df.shape[1]))
    if df.shape[1] != df_sus.shape[1]:
        print("There are a different number of columns!")

    for index in df.columns:
        if index not in df_sus:
            print(f"First dataframe has '{index}' which NOT in the second dataframe")

    for index in df_sus.columns:
        if index not in df:
            print(f"Second dataframe has '{index}' which NOT in the first dataframe")

    return

def check_dataframes_and_equalize(df: pd.DataFrame, df_sus: pd.DataFrame):
    print("First Dataframe colm count: " + str(df.shape[1]))
    if df.shape[1] != df_sus.shape[1]:
        print("There are a different number of columns!")

    for index in df.columns:
        if index not in df_sus:
            print(f"First dataframe has '{index}' which NOT in the second dataframe. Dropping...")
            df.drop(columns=[index], inplace=True)

    for index in df_sus.columns:
        if index not in df:
            print(f"Second dataframe has '{index}' which NOT in the first dataframe. Dropping...")
            df_sus.drop(columns=[index], inplace=True)

    return df, df_sus

if predict_files_flag:
    logging.info(f"Predicting samples from {predict_files_folder}")
    print(f"Predicting samples from {predict_files_folder}")

    df_predict = get_features_from_files_in_path(predict_files_folder, "predict")
    df_data = df_predict.drop(columns=["label"])
    if "function_names" in df_data:
        logging.info("Dropping Function names from features list")
        df_data.drop(columns=["function_names"], inplace=True)
    if "extracted_vba" in df_data:
        logging.info("Dropping extracted_vba from features list")
        df_data.drop(columns=["extracted_vba"], inplace=True)
    if "filename_vba" in df_data:
        logging.info("Dropping filename_vba from features list")
        df_data.drop(columns=["filename_vba"], inplace=True)
    if "stream_path" in df_data:
        logging.info("Dropping stream_path from features list")
        df_data.drop(columns=["stream_path"], inplace=True)
    if "file_hash" in df_data:
        logging.info("Dropping file hash from features list")
        df_data.drop(columns=["file_hash"], inplace=True)
    #
    #print("First DataFrame is df_full")
    #check_dataframes(df_full, df_data)

    # I don't know how or why but the df_full got extra column's
    #df_full.drop('count_76487-337-8429955-22614', inplace=True)
    #df_full.drop('count_1824245000', inplace=True)
    #df_full.drop('tfidf_76487-337-8429955-22614', inplace=True)
    #df_full.drop('tfidf_1824245000', inplace=True)

    # check_dataframes_and_equalize(df_full, df_data)
    #check_dataframes(df_full, df_data)
    # I have NO IDEA where these features came from. Every time I re-train the model on the same data, they keep popping up
    if "count_76487-337-8429955-22614" in df_data:
        logging.info("Dropping count_76487-337-8429955-22614 from features list")
        df_data.drop(columns=["count_76487-337-8429955-22614"], inplace=True)
    if "count_1824245000" in df_data:
        logging.info("Dropping count_1824245000 from features list")
        df_data.drop(columns=["count_1824245000"], inplace=True)
    if "tfidf_76487-337-8429955-22614" in df_data:
        logging.info("Dropping tfidf_76487-337-8429955-22614 from features list")
        df_data.drop(columns=["tfidf_76487-337-8429955-22614"], inplace=True)
    if "tfidf_1824245000" in df_data:
        logging.info("Dropping tfidf_1824245000 from features list")
        df_data.drop(columns=["tfidf_1824245000"], inplace=True)


    np_predicted = forest.predict(df_data)
    np_predicted_prop = forest.predict_proba(df_data)

    for value, row, probability in zip(np_predicted, df_predict[["file_hash"]].values, np_predicted_prop):
        print(value,",", round(probability.max(), 2), ",", probability ,",", row)

    print("Done Predicting")


if save_model_flag:
    save_model(forest)


if len(df_full.index) == 0:
    logging.info("Original Features not present. Exiting without generating graphic  ")
    exit()

# TODO: Do more analysis. Like "How do I test to see how good my Forrest is performming?
# TODO: Maybe I can graph new samples with varying levels of detection
importances = forest.feature_importances_
indices = np.argsort(importances)[::-1]

# Drop the rows just in case
if "function_names" in df_full:
    df_full.drop(columns=["function_names"], inplace=True)
if "extracted_vba" in df_full:
    df_full.drop(columns=["extracted_vba"], inplace=True)
if "filename_vba" in df_full:
    df_full.drop(columns=["filename_vba"], inplace=True)
if "stream_path" in df_full:
    df_full.drop(columns=["stream_path"], inplace=True)
if "file_hash" in df_full:
    df_full.drop(columns=["file_hash"], inplace=True)

feature_names = []
feature_names_raw = df_full.columns[:-1]
for name in feature_names_raw:
    feature_names.append(name.replace("_", " ").title().replace("Vba", "VBA").replace("Tfidf","TF-IDF"))
forest_importances = pd.Series(importances, index=feature_names)

count_of_total_samples = len(df_full.index)
count_with_commas = f'{count_of_total_samples:,}'
count_mal_samples = df_full[df_full['label'] == 'malicious'].shape[0]
count_benign_samples = df_full[df_full['label'] == 'benign'].shape[0]

std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)

feature_count = len(df_full.columns)

# Plotting the top 10 most important features
indices = np.argsort(importances)[::-1]
top_n = 20  # You can adjust this number as needed
plt.figure(figsize=(10, 6))
plt.title(f"Top {top_n} Feature Importances ({count_with_commas} samples {count_mal_samples} mal to {count_benign_samples} good)")
plt.bar(range(top_n), importances[indices[:top_n]], align="center")
plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=90)
plt.xlabel(f"Features ({feature_count})")
plt.ylabel(f"Importance across {number_of_trees} trees")
plt.subplots_adjust(bottom=0.4, top=0.90)
filename = os.path.join(image_path,generate_log_filename("Figure_1")).replace("gz","png")
plt.savefig(filename, format = 'png', dpi=300)
plt.show()

# MDI
#fig, ax = plt.subplots()
#forest_importances.plot.bar(yerr=std, ax=ax)
#ax.set_title("Feature Importances using MDI")
#ax.set_ylabel("Mean Decrease")
#fig.tight_layout()
#plt.show()

# MDI with error
#result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=100)
#fig, ax = plt.subplots()
#forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
#ax.set_title("Feature Importances using permutation on full model")
#ax.set_ylabel("Mean accuracy decrease")
#fig.tight_layout()
#plt.show()

# Right side up
#heights = list(forest_importances)
#x_pos = np.arange(len(feature_names))
#plt.bar(x_pos, heights)
#plt.xticks(x_pos, feature_names)
#plt.yticks()
#plt.xticks(x_pos, feature_names, rotation=90)
#plt.subplots_adjust(bottom=0.5, top=0.99)
#plt.show()

#for sample in global_failed_sample_list:
#    logging.error(f"Failed to load: {sample} ")



logging.info("Done")