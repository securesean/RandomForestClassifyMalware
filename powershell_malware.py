import os
import sys
import logging
import re
import datetime
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import joblib
import glob
import hashlib
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
import time



# Config Notes:
# Data Save config:
save_directory = "saves"
# Features are saved as a dataframe
# Models are saved as a dict containing the features (dataframe from above) and a trained model
process_files_from_small_folders_flag = False


load_last_feature_flag = True
save_features_flag = False

Grid_Search_Flag = False

train_model_flag = False
train_with_all_samples_flag = False
save_model_flag = False
load_last_model_flag = True

predict_files_flag = True


# VT Searches:
## p:20+ entity:file tag:macros size:10MB- fs:2016-01-01+ fs:2023-05-01-
## Did not do well: p:1- entity:file tag:macros size:10MB- fs:2016-01-01+ fs:2023-05-01-
##     Slightly better: fs:2016-01-01+ fs:2023-05-01- type:document tag:macros size:10MB- p:1-
# for benign look past a year to make sure AV's have had time to write signatures

malware_folder = "samples_malicious"
malware_folder = "D:\\Malware\\PowerShell_malicious"
benign_folder = "samples_benign"
benign_folder = "D:\\Malware\\PowerShell_benign"
vocab_path = "vocab_powershell.txt"
image_path = "figures"
predict_files_folder = "samples_for_chart"

# Model Config
number_of_trees = 100

# Create output Folders if they don't already exist
if save_model_flag and not os.path.exists(save_directory):
    os.makedirs(save_directory)


# Globals (I know, I'm bad)
df_full = pd.DataFrame()
count_of_added_mal_samples = 0
count_of_added_mal_samples_failed = 0
count_of_added_benign_samples = 0
count_of_added_benign_samples_failed = 0
global_failed_sample_list = []



# level=logging.INFO
# level=logging.ERROR
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.info("Starting")

logging.info("Loading Vocab List")
if not os.path.exists(vocab_path):
    logging.error(f"Vocab file missing: {vocab_path}")
    exit()
with open(vocab_path) as vocabfile:
    lines = vocabfile.readlines()
    lines = [x.strip().lower() for x in lines]
vocab_list = list(set(lines))
vocab_list_temp = []
for line in vocab_list:
    if line.startswith("#"):
        continue
    vocab_list_temp.append(line)
vocab_list = vocab_list_temp
if " " in vocab_list:
    vocab_list.remove(" ")
if "" in vocab_list:
    vocab_list.remove("")
# How many interesting/reserved language words are in the VBA?
vectorizerizer = CountVectorizer(vocabulary=vocab_list,
                                     lowercase=True,
                                     decode_error='ignore',
                                     token_pattern=r"(?u)\b\w[\w\.]+\b")
def generate_log_filename(prependString = ""):
    # Current time formatted as 'YYYY-MM-DD_HH-MM-SS'
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    # Generate filename
    filename = f"{prependString}_{timestamp}.gz"
    return filename

def return_decoded_value(value):
    if type(value) is bytes:
        value = value.decode('utf-8')
    elif type(value) is not str:
        value = value.decode("ascii", "ignore")
    else:
        value = value

    return value.strip('\r\n')


def get_entropy( vbcodeSeries):
    """
    Helper function to return entropy calculation value
    :param vbcodeSeries: pandas series of values
    :return: entropy of the set of values.
    """
    probs = vbcodeSeries.value_counts() / len(vbcodeSeries)
    entropy = stats.entropy(probs)
    return entropy

def get_language_features(powershell_source):
    features = {}
    global vectorizerizer



    # This is probably a really inefficient way to do this
    extacted_vba = [powershell_source] # turn it into a list because the CountVectorizer expects a list
    word_count_matrix = vectorizerizer.fit_transform(extacted_vba)
    word_count_array = word_count_matrix.toarray()
    feature_names = vectorizerizer.get_feature_names_out()
    aggregate_counts = word_count_array.sum(axis=0)
    frequency_dict = dict(zip("count_" + feature_names, aggregate_counts))

    # Add to our main dictionary of features
    features.update(frequency_dict)
    #for word in vectorizerizer.get_feature_names_out():
    #    features["count_" + word] = vectorizerizer.vocabulary_.get(word)


    # TF-IDF Transformer
    # Interesting word frequency per document (VBA)
    tf_transformer = TfidfTransformer()
    tfidf_matrix = tf_transformer.fit_transform(word_count_matrix)
    tfidf_array = tfidf_matrix.toarray()
    aggregated_tfidf = tfidf_array.sum(axis=0)
    tfidf_dict = dict(zip("tfidf_" + feature_names, aggregated_tfidf))
    # Add to our main dictionary of features
    features.update(tfidf_dict)

    #word_freq_matrix = model_tfidf_trans.fit_transform(word_count_matrix.toarray())
    #for word in model_tfidf_trans.get_feature_names_out():
    #    features["tfidf_" + word] = model_tfidf_trans.vocabulary_.get(word)

    # Get all other language features
    features.update(get_vba_features(powershell_source))
    return features

def load_model():
    try:
        dat_files = glob.glob(os.path.join(save_directory, '*model*.gz'))
        latest_file = max(dat_files, key=os.path.getctime, default=None)

        if latest_file is not None:
            print("Loading Model from " + latest_file)
            logging.info("Loading Model from " + latest_file)
            saved_data = joblib.load(latest_file)
            return saved_data

        if latest_file is None:
            message = "Error loading models from disk: No latest file found"
            logging.error(message)
            raise IOError(message)

    except TypeError as y:
        message = "Pickle file may be corrupted, please verify you have a proper pickle file {}".format(str(y))
        logging.error(message)
        raise IOError(message)

    except Exception as e:
        message = "Error loading models from disk: {}".format(str(e))
        logging.error(message)
        raise IOError(message)

def load_features():
    try:
        dat_files = glob.glob(os.path.join(save_directory, '*feature*.gz'))
        latest_file = max(dat_files, key=os.path.getctime, default=None)
        if latest_file is not None:
            print("Loading Features from " + latest_file)
            logging.info("Loading Features from " + latest_file)
            df_features = joblib.load(latest_file)
            return df_features
        if latest_file is None:
            message = "Error loading features from disk: No latest file found"
            logging.error(message)
            raise IOError(message)

    except TypeError as y:
        message = "Pickle file may be corrupted, please verify you have a proper pickle file {}".format(str(y))
        logging.error(message)
        raise IOError(message)

    except Exception as e:
        message = "Error loading features from disk: {}".format(str(e))
        logging.error(message)
        raise IOError(message)

def save_features(features):
    try:
        savefile = generate_log_filename("features")
        savefile = os.path.join(save_directory, savefile)
        logging.info("Saving Features " + savefile)
        joblib.dump(features, savefile, compress="gzip")
    except Exception as e:
        message = "Error saving model data to disk: {}".format(str(e))
        logging.error(message)
def save_model(model):
    global number_of_trees
    try:
        savefile = generate_log_filename(f"model_{number_of_trees}_trees")
        savefile = os.path.join(save_directory, savefile)
        logging.info(f"Saving Model to {savefile} " )
        print(f"Saving Model to {savefile} ")
        joblib.dump(model, savefile, compress="gzip")
    except Exception as e:
        message = "Error saving model data to disk: {}".format(str(e))
        logging.error(message)

def get_vba_features(vb):
    """
    Given VB code as a string input, returns various summary data about it.
    :param vb: vbacode as one large multiline string
    :return: pandas Series that can be used in concert with the pandas DataFrame apply method
    """
    allfunctions = []
    all_num_functions = []
    all_locs = []
    entropy_func_names = 0
    avg_param_per_func = 0.0
    functions_str = ''
    vba_cnt_func_loc_ratio = 0.0
    vba_cnt_comment_loc_ratio = 0.0

    if vb == 'No VBA Macros found' or vb[0:6] == 'Error:':
        functions = 'None'
        num_functions = 0
        loc = 0
        avg_loc_func = 0
        num_comments = 0
        entropy_chars = 0
        entropy_words = 0
    else:
        functions = {}
        num_comments = vb.count("#")  # TODO: do a better job
        lines = vb.splitlines()
        new_lines = []
        num_functions = 0
        entropy_chars = get_entropy(pd.Series(vb.split(' ')))
        entropy_words = get_entropy(pd.Series(list(vb)))
        reFunction = re.compile(r'function\s+([A-Za-z0-9_-]*)\s*\(([^\)]*)\)\s*{')
        for line in lines:
            if len(line.strip()) > 0:
                #print(f"\t{line}")
                new_lines.append(line)

        function_name_matches = reFunction.findall(line)
        num_params = 0
        if len(function_name_matches) > 0:
            num_functions = num_functions + 1
            num_params = function_name_matches[0][1].count(',') + 1
            if len(function_name_matches[0][1].strip()) <= 0:           # ToDO: account for the param() code block.
                num_params = 0
            functions[function_name_matches[0][0]] = num_params

        loc = len(new_lines)
        if len(functions) > 0:
            function_name_str = ''.join(functions.keys())
            entropy_func_names = get_entropy(pd.Series(list(function_name_str)))
            functions_str = ', '.join(functions.keys())
            param_list = functions.values()
            avg_param_per_func = (1.0 * sum(param_list)) / len(param_list)
        if loc > 0:
            vba_cnt_func_loc_ratio = (1.0 * len(functions)) / loc
            vba_cnt_comment_loc_ratio = (1.0 * num_comments) / loc
        if num_functions <= 0:
            avg_loc_func = float(loc)
        else:
            avg_loc_func = float(loc) / num_functions

    return {'function_names': functions_str,
                      'ps_avg_param_per_func': avg_param_per_func,
                      'ps_cnt_comments': num_comments,
                      'ps_cnt_loc': loc,
                      'ps_cnt_func_loc_ratio': vba_cnt_func_loc_ratio,
                      'ps_cnt_comment_loc_ratio': vba_cnt_comment_loc_ratio,
                      'ps_entropy_chars': entropy_chars,
                      'ps_entropy_words': entropy_words,
                      'ps_entropy_func_names': entropy_func_names,
                      'ps_mean_loc_per_func': avg_loc_func
                      }

def get_features_from_files_in_path(folderPath, label):
    '''
    :param folderPath: Path to the folder containing the files
    :param label: "malicious" or "benign"
    :return: Df containing all the data
    '''
    global df_full
    logging.info(f"Loading {label} Samples from {folderPath}")
    if label == "malicious" or label == "benign" or label == "predict":
        pass
    else:
        logging.error(f"Label does not have proper string: {label}")
        exit()

    # Making a list of dict's because growing a dataframe row-by-row is (apparently) terrible
    list_of_dict_features = []
    failed_sample_list = []
    if not os.path.exists(folderPath):
        logging.error(f"Can not load samples - folder does not exist: {folderPath} ")
        return pd.DataFrame()
    for file in os.listdir(folderPath):
        full_path = os.path.join(folderPath, file)

        # Check to see if we already have processed this file. TODO: check to see how effienct this is
        file_hash = getFileNameHash(full_path)
        exists = False
        if len(df_full.index) != 0:
            exists = ((df_full['label'] == label) & (df_full['file_hash'] == file_hash)).any()
        if exists:
            logging.info(f"Skipping already processed: {full_path}")
            continue
        logging.info("Loading: " + full_path)
        try:
            code = open(full_path).read()
        except Exception as e:
            if "utf-16-le" in str(e):
                logging.info("Attempting to decode as utf-16-le")
                code = open(full_path, encoding='utf-16').read()
            else:
                logging.warning(f"Unknown encoding for: {full_path} . Will attempt to read it anyway")
                try:
                    code = open(full_path, errors='ignore').read()
                except Exception as f:
                    logging.warning(f"Failed to load: {full_path} Error: {str(f)}")
                    continue

        logging.info(f"Getting Language features: {full_path}")
        dict_features = get_language_features(code)
        logging.info(f"Done getting Language features: {full_path}")
        dict_features['file_hash'] = file_hash
        dict_features['label'] = label
        list_of_dict_features.append(dict_features)

    count_of_added_samples = len(list_of_dict_features)
    count_of_failed_samples = len(failed_sample_list)
    logging.info(f"Samples ({label}) processed: {count_of_added_samples}")
    logging.info(f"Samples ({label}) failed to be processed: {count_of_failed_samples}")

    # update global counts
    global count_of_added_mal_samples, count_of_added_mal_samples_failed, count_of_added_benign_samples, count_of_added_benign_samples_failed, global_failed_sample_list
    if label == "malicious":
        count_of_added_mal_samples += count_of_added_samples
        count_of_added_mal_samples_failed += count_of_failed_samples
        logging.info(f"Loaded {count_of_added_mal_samples} malicious samples")
    elif label == "benign":
        count_of_added_benign_samples += count_of_added_samples
        count_of_added_benign_samples_failed += count_of_failed_samples
        logging.info(f"Loaded {count_of_added_benign_samples} benign samples")
    elif label == "predict":
        logging.info(f"Predicted Sample. ")
    else:
        logging.error(f"Label does not have proper string: {label}")
        exit()

    global_failed_sample_list = failed_sample_list + global_failed_sample_list
    # Combine all the data
    df_ret = pd.DataFrame(list_of_dict_features)
    return df_ret

def getFileNameHash(full_path):
    # assume if the file is named with sha256, that it's correct, else this will slow down analysis
    filenameToHash = os.path.basename(full_path)
    pattern = r'^[a-fA-F0-9]{64}$'
    match = re.match(pattern, filenameToHash)
    if match is not None:
        return filenameToHash.lower()
    else:
        logging.info(f"Hashing {full_path}")
        #print(f"Hashing {full_path}")
        sha256_hash = hashlib.sha256()
        try:
            with open(full_path, "rb") as sampleToHash:
                # Read and update hash string value in blocks of 4K
                for byte_block in iter(lambda: sampleToHash.read(4096), b""):
                    sha256_hash.update(byte_block)
        except FileNotFoundError:
            logging.error(f"Failed to hash file because it was not found: {full_path}")
            exit()
        except Exception as e:
            logging.error(f"Failed to hash file: {str(e)}")
            exit()

    logging.info(f"Hashed to {sha256_hash.hexdigest().lower()}")
    return sha256_hash.hexdigest().lower()



####################################### Main #######################################
if load_last_feature_flag:
    if load_last_feature_flag:
        logging.info("Loading Features from file")
        df_loaded = load_features()
        logging.info(f"Loaded {len(df_loaded.index)} rows from file")
        df_full = pd.concat([df_full, df_loaded], ignore_index=True, sort=False)

if process_files_from_small_folders_flag:
    start_time = time.time()
    df_mal = get_features_from_files_in_path(malware_folder, "malicious")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Loading {malware_folder} completed. It took {elapsed_time:.2f} seconds.")
    df_full = pd.concat([df_full, df_mal], ignore_index=True, sort=False)

    start_time = time.time()
    df_benign = get_features_from_files_in_path(benign_folder,"benign")
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Loading {benign_folder} completed. It took {elapsed_time:.2f} seconds.")
    df_full = pd.concat([df_full, df_benign], ignore_index=True, sort=False)


    logging.info(f"New Malicious Samples: {count_of_added_mal_samples}")
    logging.info(f"New Benign Samples: {count_of_added_benign_samples}")
    print(f"New Malicious Samples: {count_of_added_mal_samples}")
    print(f"New Benign Samples: {count_of_added_benign_samples}")

if save_features_flag:
    save_features(df_full)

if load_last_model_flag:
    forest = load_model()

if train_model_flag:
    if load_last_model_flag:
        logging.warning("Flags set for Training a Model AND loading a model from disk")
    logging.info(f"Preparing data to train on {len(df_full.index)} rows ")

    if "function_names" in df_full:
        logging.info("Dropping Function names from features list")
        df_full.drop(columns=["function_names"], inplace=True)
    if "file_hash" in df_full:
        logging.info("Dropping file hash from features list")
        df_full.drop(columns=["file_hash"], inplace=True)

    # Data Slicing
    X = df_full.values[:, : -1]  # removing the last column's (the feature we want to predict which is 'malicious' or 'benign')
    Y = df_full.values[:, -1]
    if len(np.unique(Y)) != 2:
        logging.error("The dataset does not have two labels: " + str(np.unique(Y)))
        exit()

    forest = RandomForestClassifier(n_estimators=number_of_trees, max_features=.2, n_jobs=-1)
    logging.info("Training...")
    start_time = time.time()
    if train_with_all_samples_flag:
        forest.fit(X, Y)
    else:
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,
                                                            random_state=100)
        forest.fit(X_train, y_train)
        y_pred = forest.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy:", accuracy)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Training completed. It took {elapsed_time:.2f} seconds for {number_of_trees} decision trees.")



def check_dataframes(df: pd.DataFrame, df_sus: pd.DataFrame):
    print("First Dataframe colm count: " + str(df.shape[1]))
    if df.shape[1] != df_sus.shape[1]:
        print("There are a different number of columns!")

    for index in df.columns:
        if index not in df_sus:
            print(f"First dataframe has '{index}' which NOT in the second dataframe")

    for index in df_sus.columns:
        if index not in df:
            print(f"Second dataframe has '{index}' which NOT in the first dataframe")

    return

if predict_files_flag:
    logging.info(f"Predicting samples from {predict_files_folder}")
    print(f"Predicting samples from {predict_files_folder}")

    df_predict = get_features_from_files_in_path(predict_files_folder, "predict")
    df_data = df_predict.copy()
    if "function_names" in df_data:
        logging.info("Dropping Function names from features list")
        df_data.drop(columns=["function_names"], inplace=True)
    if "file_hash" in df_data:
        logging.info("Dropping file hash from features list")
        df_data.drop(columns=["file_hash"], inplace=True)

    df_data.drop( columns=[ "label"], inplace=True)
    np_predicted = forest.predict(df_data)
    np_predicted_prop = forest.predict_proba(df_data)

    for value, row, probability in zip(np_predicted, df_predict[["file_hash"]].values, np_predicted_prop):
        print(value,",", probability ,",", row) #  round(probability.max(), 2),

    print("Done Predicting")



if save_model_flag:
    save_model(forest)


if len(df_full.index) == 0:
    logging.info("Original Features not present. Exiting without generating graphic  ")
    exit()

# TODO: Do more analysis. Like "How do I test to see how good my Forrest is performming?
# TODO: Maybe I can graph new samples with varying levels of detection
importances = forest.feature_importances_
indices = np.argsort(importances)[::-1]

# Drop the rows just in case
if "file_hash" in df_full:
    df_full.drop(columns=["file_hash"], inplace=True)

feature_names = []
feature_names_raw = df_full.columns[:-1]
for name in feature_names_raw:
    feature_names.append(name.replace("_", " ").title().replace("Vba", "VBA").replace("Tfidf","TF-IDF"))
forest_importances = pd.Series(importances, index=feature_names)

count_of_total_samples = len(df_full.index)
count_with_commas = f'{count_of_total_samples:,}'
count_mal_samples = df_full[df_full['label'] == 'malicious'].shape[0]
count_benign_samples = df_full[df_full['label'] == 'benign'].shape[0]

std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)

feature_count = len(df_full.columns)

# Plotting the top 10 most important features
indices = np.argsort(importances)[::-1]
top_n = 20  # You can adjust this number as needed
plt.figure(figsize=(10, 6))
plt.title(f"Top {top_n} Feature Importances ({count_with_commas} samples {count_mal_samples} mal to {count_benign_samples} good)")
plt.bar(range(top_n), importances[indices[:top_n]], align="center")
plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=90)
plt.xlabel(f"Features ({feature_count})")
plt.ylabel(f"Importance across {number_of_trees} trees")
plt.subplots_adjust(bottom=0.4, top=0.90)
filename = os.path.join(image_path,generate_log_filename("Figure_1")).replace("gz","png")
plt.savefig(filename, format = 'png', dpi=300)
plt.show()




logging.info("Done")