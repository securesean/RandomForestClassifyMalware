{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa564bc0-f634-424e-b012-ef54ec898305",
   "metadata": {},
   "source": [
    "## Malicious Macro Classification program\n",
    "This notebook is based on the vba_malware.py source code file. Any actual training, testing, and predicting should be done with that file. This notebook does not guarantee that it does not have bugs and is working out-of-the-box. See https://github.com/securesean/RandomForestClassifyMalware for the latest version of that script, the extracted features, and the models. The code below is a demonstration of the methodology used to determine if a given document is likely malicious via extracting and analyzing VBA (Visual Basic for Applications), extracting it's features, and applying a trained model based on data from VirusTotal.com. If needed, download the model from here: https://1drv.ms/u/s!AmizhWxixAVKx-9i7VZRPl8oHUXifg?e=K2RujK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2106a15-d0aa-40d4-b361-fb5770a430b2",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports: \n",
    "The script begins by importing necessary Python libraries for file handling, data manipulation, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcb149-b0d3-4644-89c9-dd3bf5b7f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import joblib\n",
    "import glob\n",
    "import hashlib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "if sys.version_info >= (3, 0):\n",
    "    from oletools.olevba3 import VBA_Parser\n",
    "else:\n",
    "    from oletools.olevba import VBA_Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223bfe4-6fd1-468a-ae73-cc2a87315c7e",
   "metadata": {},
   "source": [
    "Configuration Details and flag variables. Various flags and paths are set up to control the flow of execution (e.g., whether to save models or features) and to define where data should be loaded from or saved to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398a3d1-e422-4293-8033-e5a166587b65",
   "metadata": {},
   "source": [
    "# Data Save config:\n",
    "save_directory = \"saves\"\n",
    "# Features are saved as a dataframe\n",
    "# Models are saved as a dict containing the features (dataframe from above) and a trained model\n",
    "process_files_from_small_folders_flag = False\n",
    "process_files_from_big_folders_flag = False\n",
    "\n",
    "save_vba_flag = False\n",
    "load_last_feature_flag = True\n",
    "save_features_flag = False\n",
    "\n",
    "Grid_Search_Flag = False\n",
    "\n",
    "train_model_flag = False\n",
    "train_with_all_samples_flag = True\n",
    "save_model_flag = False\n",
    "load_last_model_flag = True\n",
    "\n",
    "predict_files_flag = False\n",
    "\n",
    "# Model Config\n",
    "number_of_trees = 100\n",
    "\n",
    "# Create output Folders if they don't already exist\n",
    "if save_model_flag and not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "if save_vba_flag and not os.path.exists(save_vba_folder):\n",
    "    os.makedirs(save_vba_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f4f9b-c967-4cf5-8501-b3d2bbe94422",
   "metadata": {},
   "source": [
    "## Global Variables and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f3b62-de00-4d70-82cf-456ca0fb238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame()\n",
    "count_of_added_mal_samples = 0\n",
    "count_of_added_mal_samples_failed = 0\n",
    "count_of_added_benign_samples = 0\n",
    "count_of_added_benign_samples_failed = 0\n",
    "global_failed_sample_list = []\n",
    "\n",
    "# level=logging.INFO\n",
    "# level=logging.ERROR\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Starting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2a4e6-0243-4a41-9924-cf7e09cae74a",
   "metadata": {},
   "source": [
    "## Loading the VocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd58a4-39d0-4849-8046-072b6c7dc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Loading Vocab List\")\n",
    "if not os.path.exists(vocab_path):\n",
    "    logging.error(f\"Vocab file missing: {vocab_path}\")\n",
    "    exit()\n",
    "with open(vocab_path) as vocabfile:\n",
    "    lines = vocabfile.readlines()\n",
    "    lines = [x.strip().lower() for x in lines]\n",
    "vocab_list = list(set(lines))\n",
    "if \" \" in vocab_list:\n",
    "    vocab_list.remove(\" \")\n",
    "if \"\" in vocab_list:\n",
    "    vocab_list.remove(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564db672-34d0-4c3b-9d21-4c88cd1568ae",
   "metadata": {},
   "source": [
    "## Function Definitions:\n",
    " - File Handling: Functions are defined for handling files, including checking their existence, reading, and writing. Thereâ€™s also functionality to hash file names for uniqueness.\n",
    " - VBA Parsing and Feature Extraction: Functions are provided to extract VBA code from files, compute entropy, and create feature vectors using term frequency-inverse document frequency (TF-IDF) and count vectorization based on a predefined vocabulary.\n",
    " - Machine Learning: Functions to load, save, and train machine learning models, specifically using a RandomForestClassifier. This includes configuring the model, performing grid search for hyperparameter tuning, and predicting using trained models.\n",
    " - Data Management: Functions to load and combine features from different datasets, handle data frames, and ensure they are correctly formatted for machine learning tasks.\n",
    " - Utility functions such a log file name generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c5fdd-9854-43da-a66d-f16f5467182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_filename(prependString = \"\"):\n",
    "    # Current time formatted as 'YYYY-MM-DD_HH-MM-SS'\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # Generate filename\n",
    "    filename = f\"{prependString}_{timestamp}.gz\"\n",
    "    return filename\n",
    "\n",
    "def return_decoded_value(value):\n",
    "    if type(value) is bytes:\n",
    "        value = value.decode('utf-8')\n",
    "    elif type(value) is not str:\n",
    "        value = value.decode(\"ascii\", \"ignore\")\n",
    "    else:\n",
    "        value = value\n",
    "\n",
    "    return value.strip('\\r\\n')\n",
    "\n",
    "def get_vba( myfile, source='filepath'):\n",
    "    \"\"\"\n",
    "    Given a file, parses out the stream paths, vba code, and vba filenames for each.\n",
    "    :param myfile: filename or data\n",
    "    :param source: type of data being passed in.  Either \"filepath\" to indicate we need to read from disk or\n",
    "    \"filecontents\" (or anything else) meaning that the file contents are being passed as a parameter.\n",
    "    :return: Dict {'extracted_vba': allcode, 'stream_path': pathnames, 'filename_vba': filenames}\n",
    "    \"\"\"\n",
    "    if source == 'filepath':\n",
    "        filedata = open(myfile, 'rb').read()\n",
    "    else:\n",
    "        filedata = myfile\n",
    "\n",
    "    try:\n",
    "        vbaparser = VBA_Parser('doesnotexist-passed by filedata param', data=filedata)\n",
    "        pathnames = ''\n",
    "        if vbaparser.detect_vba_macros():\n",
    "            filenameslist = []\n",
    "            pathnameslist = []\n",
    "            vbacodelist = []\n",
    "            for (filename, stream_path, filename_vba, extracted_vba) in vbaparser.extract_macros():\n",
    "                vbacodelist.append(return_decoded_value(extracted_vba))\n",
    "                if not pathnames:\n",
    "                    pathnameslist.append(return_decoded_value(stream_path))\n",
    "                    filenameslist.append(return_decoded_value(filename_vba))\n",
    "                else:\n",
    "                    pathnameslist.append(return_decoded_value(stream_path))\n",
    "                    filenameslist.append(return_decoded_value(filename_vba))\n",
    "            allcode = \"\\n\\n\\n\\n\".join(vbacodelist)\n",
    "            filenames = \", \".join(filenameslist)\n",
    "            pathnames = \", \".join(pathnameslist)\n",
    "\n",
    "        else:\n",
    "            pathnames = 'No VBA Macros found'\n",
    "            filenames = 'No VBA Macros found'\n",
    "            allcode = 'No VBA Macros found'\n",
    "\n",
    "    except Exception as e:\n",
    "        pathnames = 'Error:' + str(e)\n",
    "        filenames = 'Error:' + str(e)\n",
    "        allcode = 'Error:' + str(e)\n",
    "        logging.error(f\"Error in filenames/code {myfile} {str(e)}\")\n",
    "\n",
    "    if save_vba_flag:\n",
    "        # TODO: Try/Except like other file write operations. maybe check to see if file already exists\n",
    "        # coming to us with the pre-pended malware_folder so we have to remove that to get the\n",
    "        # This is slow, if I need to speed it up I can use the job lib\n",
    "        original_file_name = os.path.basename(myfile)\n",
    "        output_path = os.path.join(save_vba_folder, original_file_name)\n",
    "        logging.info(f\"Saving VBA output to {output_path}\")\n",
    "        open(output_path, \"w\").write(allcode)\n",
    "\n",
    "    return {'extracted_vba': allcode, 'stream_path': pathnames, 'filename_vba': filenames}\n",
    "\n",
    "def get_entropy( vbcodeSeries):\n",
    "    \"\"\"\n",
    "    Helper function to return entropy calculation value\n",
    "    :param vbcodeSeries: pandas series of values\n",
    "    :return: entropy of the set of values.\n",
    "    \"\"\"\n",
    "    probs = vbcodeSeries.value_counts() / len(vbcodeSeries)\n",
    "    entropy = stats.entropy(probs)\n",
    "    return entropy\n",
    "\n",
    "def get_language_features(vba_source):\n",
    "    features = {}\n",
    "\n",
    "    # How many interesting/reserved language words are in the VBA?\n",
    "    vectorizerizer = CountVectorizer(vocabulary=vocab_list,\n",
    "                                         lowercase=True,\n",
    "                                         decode_error='ignore',\n",
    "                                         token_pattern=r\"(?u)\\b\\w[\\w\\.]+\\b\")\n",
    "\n",
    "    # This is probably a really inefficient way to do this\n",
    "    extacted_vba = [vba_source] # turn it into a list because the CountVectorizer expects a list\n",
    "    word_count_matrix = vectorizerizer.fit_transform(extacted_vba)\n",
    "    word_count_array = word_count_matrix.toarray()\n",
    "    feature_names = vectorizerizer.get_feature_names_out()\n",
    "    aggregate_counts = word_count_array.sum(axis=0)\n",
    "    frequency_dict = dict(zip(\"count_\" + feature_names, aggregate_counts))\n",
    "\n",
    "    # Add to our main dictionary of features\n",
    "    features.update(frequency_dict)\n",
    "    #for word in vectorizerizer.get_feature_names_out():\n",
    "    #    features[\"count_\" + word] = vectorizerizer.vocabulary_.get(word)\n",
    "\n",
    "\n",
    "    # TF-IDF Transformer\n",
    "    # Interesting word frequency per document (VBA)\n",
    "    tf_transformer = TfidfTransformer()\n",
    "    tfidf_matrix = tf_transformer.fit_transform(word_count_matrix)\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "    aggregated_tfidf = tfidf_array.sum(axis=0)\n",
    "    tfidf_dict = dict(zip(\"tfidf_\" + feature_names, aggregated_tfidf))\n",
    "    # Add to our main dictionary of features\n",
    "    features.update(tfidf_dict)\n",
    "\n",
    "    #word_freq_matrix = model_tfidf_trans.fit_transform(word_count_matrix.toarray())\n",
    "    #for word in model_tfidf_trans.get_feature_names_out():\n",
    "    #    features[\"tfidf_\" + word] = model_tfidf_trans.vocabulary_.get(word)\n",
    "\n",
    "    # Get all other language features\n",
    "    features.update(get_vba_features(vba_source))\n",
    "    return features\n",
    "\n",
    "def load_model():\n",
    "    try:\n",
    "        dat_files = glob.glob(os.path.join(save_directory, '*model*.gz'))\n",
    "        latest_file = max(dat_files, key=os.path.getctime, default=None)\n",
    "\n",
    "        if latest_file is not None:\n",
    "            print(\"Loading Model from \" + latest_file)\n",
    "            logging.info(\"Loading Model from \" + latest_file)\n",
    "            saved_data = joblib.load(latest_file)\n",
    "            return saved_data\n",
    "\n",
    "        if latest_file is None:\n",
    "            message = \"Error loading models from disk: No latest file found\"\n",
    "            logging.error(message)\n",
    "            raise IOError(message)\n",
    "\n",
    "    except TypeError as y:\n",
    "        message = \"Pickle file may be corrupted, please verify you have a proper pickle file {}\".format(str(y))\n",
    "        logging.error(message)\n",
    "        raise IOError(message)\n",
    "\n",
    "    except Exception as e:\n",
    "        message = \"Error loading models from disk: {}\".format(str(e))\n",
    "        logging.error(message)\n",
    "        raise IOError(message)\n",
    "\n",
    "def load_features():\n",
    "    try:\n",
    "        dat_files = glob.glob(os.path.join(save_directory, '*feature*.gz'))\n",
    "        latest_file = max(dat_files, key=os.path.getctime, default=None)\n",
    "        if latest_file is not None:\n",
    "            print(\"Loading Features from \" + latest_file)\n",
    "            logging.info(\"Loading Features from \" + latest_file)\n",
    "            df_features = joblib.load(latest_file)\n",
    "            return df_features\n",
    "        if latest_file is None:\n",
    "            message = \"Error loading features from disk: No latest file found\"\n",
    "            logging.error(message)\n",
    "            raise IOError(message)\n",
    "\n",
    "    except TypeError as y:\n",
    "        message = \"Pickle file may be corrupted, please verify you have a proper pickle file {}\".format(str(y))\n",
    "        logging.error(message)\n",
    "        raise IOError(message)\n",
    "\n",
    "    except Exception as e:\n",
    "        message = \"Error loading features from disk: {}\".format(str(e))\n",
    "        logging.error(message)\n",
    "        raise IOError(message)\n",
    "\n",
    "def save_features(features):\n",
    "    try:\n",
    "        savefile = generate_log_filename(\"features\")\n",
    "        savefile = os.path.join(save_directory, savefile)\n",
    "        logging.info(\"Saving Features \" + savefile)\n",
    "        joblib.dump(features, savefile, compress=\"gzip\")\n",
    "    except Exception as e:\n",
    "        message = \"Error saving model data to disk: {}\".format(str(e))\n",
    "        logging.error(message)\n",
    "def save_model(model):\n",
    "    global number_of_trees\n",
    "    try:\n",
    "        savefile = generate_log_filename(f\"model_{number_of_trees}_trees\")\n",
    "        savefile = os.path.join(save_directory, savefile)\n",
    "        logging.info(f\"Saving Model to {savefile} \" )\n",
    "        print(f\"Saving Model to {savefile} \")\n",
    "        joblib.dump(model, savefile, compress=\"gzip\")\n",
    "    except Exception as e:\n",
    "        message = \"Error saving model data to disk: {}\".format(str(e))\n",
    "        logging.error(message)\n",
    "\n",
    "def get_vba_features(vb):\n",
    "    \"\"\"\n",
    "    Given VB code as a string input, returns various summary data about it.\n",
    "    :param vb: vbacode as one large multiline string\n",
    "    :return: pandas Series that can be used in concert with the pandas DataFrame apply method\n",
    "    \"\"\"\n",
    "    allfunctions = []\n",
    "    all_num_functions = []\n",
    "    all_locs = []\n",
    "    entropy_func_names = 0\n",
    "    avg_param_per_func = 0.0\n",
    "    functions_str = ''\n",
    "    vba_cnt_func_loc_ratio = 0.0\n",
    "    vba_cnt_comment_loc_ratio = 0.0\n",
    "\n",
    "    if vb == 'No VBA Macros found' or vb[0:6] == 'Error:':\n",
    "        functions = 'None'\n",
    "        num_functions = 0\n",
    "        loc = 0\n",
    "        avg_loc_func = 0\n",
    "        num_comments = 0\n",
    "        entropy_chars = 0\n",
    "        entropy_words = 0\n",
    "    else:\n",
    "        functions = {}\n",
    "        num_comments = vb.count(\"'\")  # TODO: do a better job\n",
    "        lines = vb.splitlines()\n",
    "        new_lines = []\n",
    "        num_functions = 0\n",
    "        entropy_chars = get_entropy(pd.Series(vb.split(' ')))\n",
    "        entropy_words = get_entropy(pd.Series(list(vb)))\n",
    "        reFunction = re.compile(r'.*\\s?[Sub|Function]\\s+([a-zA-Z0-9_]+)\\((.*)\\)')\n",
    "        for line in lines:\n",
    "            if len(line.strip()) > 0:\n",
    "                new_lines.append(line)\n",
    "\n",
    "            function_name_matches = reFunction.findall(line)\n",
    "            num_params = 0\n",
    "            if len(function_name_matches) > 0:\n",
    "                num_functions = num_functions + 1\n",
    "                num_params = function_name_matches[0][1].count(',') + 1\n",
    "                if len(function_name_matches[0][1].strip()) <= 0:\n",
    "                    num_params = 0\n",
    "                functions[function_name_matches[0][0]] = num_params\n",
    "\n",
    "        loc = len(new_lines)\n",
    "        if len(functions) > 0:\n",
    "            function_name_str = ''.join(functions.keys())\n",
    "            entropy_func_names = get_entropy(pd.Series(list(function_name_str)))\n",
    "            functions_str = ', '.join(functions.keys())\n",
    "            param_list = functions.values()\n",
    "            avg_param_per_func = (1.0 * sum(param_list)) / len(param_list)\n",
    "        if loc > 0:\n",
    "            vba_cnt_func_loc_ratio = (1.0 * len(functions)) / loc\n",
    "            vba_cnt_comment_loc_ratio = (1.0 * num_comments) / loc\n",
    "        if num_functions <= 0:\n",
    "            avg_loc_func = float(loc)\n",
    "        else:\n",
    "            avg_loc_func = float(loc) / num_functions\n",
    "\n",
    "    return {'function_names': functions_str,\n",
    "                      'vba_avg_param_per_func': avg_param_per_func,\n",
    "                      'vba_cnt_comments': num_comments,\n",
    "                      'vba_cnt_functions': num_functions,\n",
    "                      'vba_cnt_loc': loc,\n",
    "                      'vba_cnt_func_loc_ratio': vba_cnt_func_loc_ratio,\n",
    "                      'vba_cnt_comment_loc_ratio': vba_cnt_comment_loc_ratio,\n",
    "                      'vba_entropy_chars': entropy_chars,\n",
    "                      'vba_entropy_words': entropy_words,\n",
    "                      'vba_entropy_func_names': entropy_func_names,\n",
    "                      'vba_mean_loc_per_func': avg_loc_func\n",
    "                      }\n",
    "\n",
    "def get_features_from_files_in_path(folderPath, label):\n",
    "    '''\n",
    "    :param folderPath: Path to the folder containing the files\n",
    "    :param label: \"malicious\" or \"benign\"\n",
    "    :return: Df containing all the data\n",
    "    '''\n",
    "    global  df_full\n",
    "    logging.info(f\"Loading {label} Samples from {folderPath}\")\n",
    "    if label == \"malicious\" or label == \"benign\" or label == \"predict\":\n",
    "        pass\n",
    "    else:\n",
    "        logging.error(f\"Label does not have proper string: {label}\")\n",
    "        exit()\n",
    "\n",
    "    # Making a list of dict's because growing a dataframe row-by-row is (apparently) terrible\n",
    "    list_of_dict_features = []\n",
    "    failed_sample_list = []\n",
    "    if not os.path.exists(folderPath):\n",
    "        logging.error(f\"Can not load samples - folder does not exist: {folderPath} \")\n",
    "        return pd.DataFrame()\n",
    "    for file in os.listdir(folderPath):\n",
    "        full_path = os.path.join(folderPath, file)\n",
    "\n",
    "        # Check to see if we already have processed this file.\n",
    "        file_hash = getFileNameHash(full_path)\n",
    "        exists = False\n",
    "        if len(df_full.index) != 0:\n",
    "            if 'file_hash' in df_full and 'label' in df_full:   # TODO: this should never happen, but for some reason it's popping up that df_full['file_hash'] doesn't exist\n",
    "                exists = ((df_full['label'] == label) & (df_full['file_hash'] == file_hash)).any()\n",
    "            else:\n",
    "                logging.warning(\"file_hash column is not in df_full, so we can't verify this file isn't already in the dataset\")\n",
    "        if exists:\n",
    "            logging.info(f\"Skipping already processed: {full_path}\")\n",
    "            continue\n",
    "        logging.info(\"Loading: \" + full_path)\n",
    "        dict_vba_code = get_vba(myfile = full_path)\n",
    "        if 'No VBA Macros found' in dict_vba_code['extracted_vba']:\n",
    "            logging.info(f\"Skipping because no VBA found in {full_path}\" )\n",
    "            failed_sample_list.append(full_path)\n",
    "        else:\n",
    "            dict_features = get_language_features(dict_vba_code['extracted_vba'])\n",
    "            dict_features['file_hash'] = file_hash\n",
    "            dict_features.update(dict_vba_code)\n",
    "            dict_features['label'] = label\n",
    "            list_of_dict_features.append(dict_features)\n",
    "\n",
    "    count_of_added_samples = len(list_of_dict_features)\n",
    "    count_of_failed_samples = len(failed_sample_list)\n",
    "    logging.info(f\"Samples ({label}) processed: {count_of_added_samples}\")\n",
    "    logging.info(f\"Samples ({label}) failed to be processed: {count_of_failed_samples}\")\n",
    "\n",
    "    # update global counts\n",
    "    global count_of_added_mal_samples, count_of_added_mal_samples_failed, count_of_added_benign_samples, count_of_added_benign_samples_failed, global_failed_sample_list\n",
    "    if label == \"malicious\":\n",
    "        count_of_added_mal_samples += count_of_added_samples\n",
    "        count_of_added_mal_samples_failed += count_of_failed_samples\n",
    "        logging.info(f\"Loaded {count_of_added_mal_samples} malicious samples\")\n",
    "    elif label == \"benign\":\n",
    "        count_of_added_benign_samples += count_of_added_samples\n",
    "        count_of_added_benign_samples_failed += count_of_failed_samples\n",
    "        logging.info(f\"Loaded {count_of_added_benign_samples} benign samples\")\n",
    "    elif label == \"predict\":\n",
    "        logging.info(f\"Predicted Sample. \")\n",
    "    else:\n",
    "        logging.error(f\"Label does not have proper string: {label}\")\n",
    "        exit()\n",
    "\n",
    "    global_failed_sample_list = failed_sample_list + global_failed_sample_list\n",
    "    # Combine all the data\n",
    "    df_ret = pd.DataFrame(list_of_dict_features)\n",
    "    return df_ret\n",
    "\n",
    "def getFileNameHash(full_path):\n",
    "    # assume if the file is named with sha256, that it's correct, else this will slow down analysis\n",
    "    filenameToHash = os.path.basename(full_path)\n",
    "    pattern = r'^[a-fA-F0-9]{64}$'\n",
    "    match = re.match(pattern, filenameToHash)\n",
    "    if match is not None:\n",
    "        return filenameToHash.lower()\n",
    "    else:\n",
    "        logging.info(f\"Hashing {full_path}\")\n",
    "        #print(f\"Hashing {full_path}\")\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        try:\n",
    "            with open(full_path, \"rb\") as sampleToHash:\n",
    "                # Read and update hash string value in blocks of 4K\n",
    "                for byte_block in iter(lambda: sampleToHash.read(4096), b\"\"):\n",
    "                    sha256_hash.update(byte_block)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Failed to hash file because it was not found: {full_path}\")\n",
    "            exit()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to hash file: {str(e)}\")\n",
    "            exit()\n",
    "\n",
    "    logging.info(f\"Hashed to {sha256_hash.hexdigest().lower()}\")\n",
    "    return sha256_hash.hexdigest().lower()\n",
    "\n",
    "def check_dataframes(df: pd.DataFrame, df_sus: pd.DataFrame):\n",
    "    print(\"First Dataframe colm count: \" + str(df.shape[1]))\n",
    "    if df.shape[1] != df_sus.shape[1]:\n",
    "        print(\"There are a different number of columns!\")\n",
    "\n",
    "    for index in df.columns:\n",
    "        if index not in df_sus:\n",
    "            print(f\"First dataframe has '{index}' which NOT in the second dataframe\")\n",
    "\n",
    "    for index in df_sus.columns:\n",
    "        if index not in df:\n",
    "            print(f\"Second dataframe has '{index}' which NOT in the first dataframe\")\n",
    "\n",
    "    return\n",
    "\n",
    "def check_dataframes_and_equalize(df: pd.DataFrame, df_sus: pd.DataFrame):\n",
    "    print(\"First Dataframe colm count: \" + str(df.shape[1]))\n",
    "    if df.shape[1] != df_sus.shape[1]:\n",
    "        print(\"There are a different number of columns!\")\n",
    "\n",
    "    for index in df.columns:\n",
    "        if index not in df_sus:\n",
    "            print(f\"First dataframe has '{index}' which NOT in the second dataframe. Dropping...\")\n",
    "            df.drop(columns=[index], inplace=True)\n",
    "\n",
    "    for index in df_sus.columns:\n",
    "        if index not in df:\n",
    "            print(f\"Second dataframe has '{index}' which NOT in the first dataframe. Dropping...\")\n",
    "            df_sus.drop(columns=[index], inplace=True)\n",
    "\n",
    "    return df, df_sus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76ea02-2d3f-4b0a-a9a3-2cf5474e167a",
   "metadata": {},
   "source": [
    "## Main Execution Flow\n",
    "The script integrates all functionalities based on conditions that check the configured flags, thus allowing for flexible execution paths like training models only if specific conditions are met, or loading pre-existing models to make predictions without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bdc08-8d8b-4019-9738-97b77da4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_last_feature_flag:\n",
    "    if load_last_feature_flag:\n",
    "        logging.info(\"Loading Features from file\")\n",
    "        df_loaded = load_features()\n",
    "        if 'file_hash' not in df_loaded:\n",
    "            logging.warning(\"Loaded Features does not contain file_hash (it should)\")\n",
    "        if 'label' not in df_loaded:\n",
    "            logging.warning(\"Loaded Features does not contain label (it should)\")\n",
    "        logging.info(f\"Loaded {len(df_loaded.index)} rows from file\")\n",
    "        df_full = pd.concat([df_full, df_loaded], ignore_index=True, sort=False)\n",
    "\n",
    "if process_files_from_small_folders_flag:\n",
    "    start_time = time.time()\n",
    "    df_mal = get_features_from_files_in_path(malware_folder, \"malicious\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Loading {malware_folder} completed. It took {elapsed_time:.2f} seconds.\")\n",
    "    df_full = pd.concat([df_full, df_mal], ignore_index=True, sort=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_benign = get_features_from_files_in_path(benign_folder,\"benign\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Loading {benign_folder} completed. It took {elapsed_time:.2f} seconds.\")\n",
    "    df_full = pd.concat([df_full, df_benign], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "    logging.info(f\"New Malicious Samples: {count_of_added_mal_samples}\")\n",
    "    logging.info(f\"New Benign Samples: {count_of_added_benign_samples}\")\n",
    "    print(f\"New Malicious Samples: {count_of_added_mal_samples}\")\n",
    "    print(f\"New Benign Samples: {count_of_added_benign_samples}\")\n",
    "\n",
    "\n",
    "\n",
    "if process_files_from_big_folders_flag:\n",
    "    start_time = time.time()\n",
    "    df_mal = get_features_from_files_in_path(malware_folder_big, \"malicious\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Loading {malware_folder_big} completed. It took {elapsed_time:.2f} seconds.\")\n",
    "    df_full = pd.concat([df_full, df_mal], ignore_index=True, sort=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_benign = get_features_from_files_in_path(benign_folder_big, \"benign\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Loading {benign_folder_big} completed. It took {elapsed_time:.2f} seconds.\")\n",
    "    df_full = pd.concat([df_full, df_benign], ignore_index=True, sort=False)\n",
    "\n",
    "    logging.info(f\"New Malicious Samples: {count_of_added_mal_samples}\")\n",
    "    logging.info(f\"New Benign Samples: {count_of_added_benign_samples}\")\n",
    "    print(f\"New Malicious Samples: {count_of_added_mal_samples}\")\n",
    "    print(f\"New Benign Samples: {count_of_added_benign_samples}\")\n",
    "\n",
    "if save_features_flag:\n",
    "    save_features(df_full)\n",
    "\n",
    "if load_last_model_flag:\n",
    "    forest = load_model()\n",
    "\n",
    "if df_full is None:\n",
    "    logging.error(\"Error Loading features. Exiting\")\n",
    "    exit()\n",
    "\n",
    "if train_model_flag:\n",
    "    if load_last_model_flag:\n",
    "        logging.warning(\"Flags set for Training a Model AND loading a model from disk\")\n",
    "    logging.info(f\"Preparing data to train on {len(df_full.index)} rows \")\n",
    "    # Remove anything I don't want and train with the DataFrame that we loaded from files one by one or from model backup\n",
    "    # This is ineffient - if I need more speed, do each of these all at once\n",
    "    start_time = time.time()\n",
    "\n",
    "    if \"function_names\" in df_full:\n",
    "        logging.info(\"Dropping Function names from features list\")\n",
    "        df_full.drop(columns=[\"function_names\"], inplace=True)\n",
    "    if \"extracted_vba\" in df_full:\n",
    "        logging.info(\"Dropping extracted_vba from features list\")\n",
    "        df_full.drop(columns=[\"extracted_vba\"], inplace=True)\n",
    "    if \"filename_vba\" in df_full:\n",
    "        logging.info(\"Dropping filename_vba from features list\")\n",
    "        df_full.drop(columns=[\"filename_vba\"], inplace=True)\n",
    "    if \"stream_path\" in df_full:\n",
    "        logging.info(\"Dropping stream_path from features list\")\n",
    "        df_full.drop(columns=[\"stream_path\"], inplace=True)\n",
    "    if \"file_hash\" in df_full:\n",
    "        logging.info(\"Dropping file hash from features list\")\n",
    "        df_full.drop(columns=[\"file_hash\"], inplace=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    # print(f\"Dropping unwanted columns took {elapsed_time:.2f} seconds.\") - only 7 or 8 seconds with a 160k rows\n",
    "\n",
    "    # Data Slicing\n",
    "    X = df_full.values[:, : -1]  # removing the last column's (the feature we want to predict which is 'malicious' or 'benign')\n",
    "    Y = df_full.values[:, -1]\n",
    "    if len(np.unique(Y)) != 2:\n",
    "        logging.error(\"The dataset does not have two labels: \" + str(np.unique(Y)))\n",
    "        exit()\n",
    "\n",
    "    forest = RandomForestClassifier(n_estimators=number_of_trees, max_features=.2, n_jobs=-1)  # TODO: Play with these hyper parameters\n",
    "    logging.info(\"Training...\")\n",
    "    # ToDo: Look into LabelBinarizer()\n",
    "    #lb = LabelBinarizer()\n",
    "    #y_train = np.array([number[0] for number in lb.fit_transform(y_train)])\n",
    "    start_time = time.time()\n",
    "    if train_with_all_samples_flag:\n",
    "        forest.fit(X, Y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,\n",
    "                                                            random_state=100)  # ToDo: play with the test values\n",
    "        forest.fit(X_train, y_train)\n",
    "        y_pred = forest.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\", accuracy)  # 0.9969434221196432\n",
    "\n",
    "        # ToDo: test these\n",
    "        # recall = cross_val_score(forest, X_train, y_train, cv=5, scoring='recall')\n",
    "        # precision = cross_val_score(forest, X_train, y_train, cv=5, scoring='precision')\n",
    "        # accuracy = cross_val_score(forest, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        # f1_score = cross_val_score(forest, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "        # print(f\"Recall {recall}\")\n",
    "        # print(f\"Precision {precision}\")\n",
    "        # print(f\"Accuracy {accuracy}\")\n",
    "        # print(f\"F1 Score {f1_score}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training completed. It took {elapsed_time:.2f} seconds for {number_of_trees} decision trees.\")\n",
    "\n",
    "if Grid_Search_Flag:\n",
    "    forest = RandomForestClassifier( max_features=.2, n_jobs=-1)\n",
    "    X = df_full.values[:,: -1]  # removing the last column's (the feature we want to predict which is 'malicious' or 'benign')\n",
    "    Y = df_full.values[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,\n",
    "                                                        random_state=100)\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 200, 500]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    # Best number of trees\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "\n",
    "    # Print the best number of trees and the corresponding accuracy\n",
    "    print(\"Best number of trees:\", best_n_estimators)\n",
    "    print(\"Best cross-validation score (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "    # Optionally, evaluate on the test set\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    test_accuracy = best_rf.score(X_test, y_test)\n",
    "    print(\"Test set accuracy:\", test_accuracy)\n",
    "\n",
    "    # Best number of trees: 500\n",
    "    # Best cross-validation score (accuracy): 0.9952775077800146\n",
    "    # Test set accuracy: 0.9957587475831098\n",
    "\n",
    "if predict_files_flag:\n",
    "    logging.info(f\"Predicting samples from {predict_files_folder}\")\n",
    "    print(f\"Predicting samples from {predict_files_folder}\")\n",
    "\n",
    "    df_predict = get_features_from_files_in_path(predict_files_folder, \"predict\")\n",
    "    df_data = df_predict.drop(columns=[\"label\"])\n",
    "    if \"function_names\" in df_data:\n",
    "        logging.info(\"Dropping Function names from features list\")\n",
    "        df_data.drop(columns=[\"function_names\"], inplace=True)\n",
    "    if \"extracted_vba\" in df_data:\n",
    "        logging.info(\"Dropping extracted_vba from features list\")\n",
    "        df_data.drop(columns=[\"extracted_vba\"], inplace=True)\n",
    "    if \"filename_vba\" in df_data:\n",
    "        logging.info(\"Dropping filename_vba from features list\")\n",
    "        df_data.drop(columns=[\"filename_vba\"], inplace=True)\n",
    "    if \"stream_path\" in df_data:\n",
    "        logging.info(\"Dropping stream_path from features list\")\n",
    "        df_data.drop(columns=[\"stream_path\"], inplace=True)\n",
    "    if \"file_hash\" in df_data:\n",
    "        logging.info(\"Dropping file hash from features list\")\n",
    "        df_data.drop(columns=[\"file_hash\"], inplace=True)\n",
    "    #\n",
    "    #print(\"First DataFrame is df_full\")\n",
    "    #check_dataframes(df_full, df_data)\n",
    "\n",
    "    # I don't know how or why but the df_full got extra column's\n",
    "    #df_full.drop('count_76487-337-8429955-22614', inplace=True)\n",
    "    #df_full.drop('count_1824245000', inplace=True)\n",
    "    #df_full.drop('tfidf_76487-337-8429955-22614', inplace=True)\n",
    "    #df_full.drop('tfidf_1824245000', inplace=True)\n",
    "\n",
    "    # check_dataframes_and_equalize(df_full, df_data)\n",
    "    #check_dataframes(df_full, df_data)\n",
    "    # I have NO IDEA where these features came from. Every time I re-train the model on the same data, they keep popping up\n",
    "    if \"count_76487-337-8429955-22614\" in df_data:\n",
    "        logging.info(\"Dropping count_76487-337-8429955-22614 from features list\")\n",
    "        df_data.drop(columns=[\"count_76487-337-8429955-22614\"], inplace=True)\n",
    "    if \"count_1824245000\" in df_data:\n",
    "        logging.info(\"Dropping count_1824245000 from features list\")\n",
    "        df_data.drop(columns=[\"count_1824245000\"], inplace=True)\n",
    "    if \"tfidf_76487-337-8429955-22614\" in df_data:\n",
    "        logging.info(\"Dropping tfidf_76487-337-8429955-22614 from features list\")\n",
    "        df_data.drop(columns=[\"tfidf_76487-337-8429955-22614\"], inplace=True)\n",
    "    if \"tfidf_1824245000\" in df_data:\n",
    "        logging.info(\"Dropping tfidf_1824245000 from features list\")\n",
    "        df_data.drop(columns=[\"tfidf_1824245000\"], inplace=True)\n",
    "\n",
    "\n",
    "    np_predicted = forest.predict(df_data)\n",
    "    np_predicted_prop = forest.predict_proba(df_data)\n",
    "\n",
    "    for value, row, probability in zip(np_predicted, df_predict[[\"file_hash\"]].values, np_predicted_prop):\n",
    "        print(value,\",\", round(probability.max(), 2), \",\", probability ,\",\", row)\n",
    "\n",
    "    print(\"Done Predicting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc257568-aa59-4cc1-ba5b-4980ba86eb0a",
   "metadata": {},
   "source": [
    "Save model if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c9ec7-4ae2-4c23-96f5-c7520f1ef7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962821e-329d-4ab7-a553-0c8249792fea",
   "metadata": {},
   "source": [
    "Generate visuals by first prepping the data then using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13e012-c1a4-45bc-9caf-db9189480db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Drop the rows just in case\n",
    "if \"function_names\" in df_full:\n",
    "    df_full.drop(columns=[\"function_names\"], inplace=True)\n",
    "if \"extracted_vba\" in df_full:\n",
    "    df_full.drop(columns=[\"extracted_vba\"], inplace=True)\n",
    "if \"filename_vba\" in df_full:\n",
    "    df_full.drop(columns=[\"filename_vba\"], inplace=True)\n",
    "if \"stream_path\" in df_full:\n",
    "    df_full.drop(columns=[\"stream_path\"], inplace=True)\n",
    "if \"file_hash\" in df_full:\n",
    "    df_full.drop(columns=[\"file_hash\"], inplace=True)\n",
    "\n",
    "feature_names = []\n",
    "feature_names_raw = df_full.columns[:-1]\n",
    "for name in feature_names_raw:\n",
    "    feature_names.append(name.replace(\"_\", \" \").title().replace(\"Vba\", \"VBA\").replace(\"Tfidf\",\"TF-IDF\"))\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "count_of_total_samples = len(df_full.index)\n",
    "count_with_commas = f'{count_of_total_samples:,}'\n",
    "count_mal_samples = df_full[df_full['label'] == 'malicious'].shape[0]\n",
    "count_benign_samples = df_full[df_full['label'] == 'benign'].shape[0]\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "feature_count = len(df_full.columns)\n",
    "\n",
    "# Plotting the top 10 most important features\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_n = 20  # You can adjust this number as needed\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"Top {top_n} Feature Importances ({count_with_commas} samples {count_mal_samples} mal to {count_benign_samples} good)\")\n",
    "plt.bar(range(top_n), importances[indices[:top_n]], align=\"center\")\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=90)\n",
    "plt.xlabel(f\"Features ({feature_count})\")\n",
    "plt.ylabel(f\"Importance across {number_of_trees} trees\")\n",
    "plt.subplots_adjust(bottom=0.4, top=0.90)\n",
    "filename = os.path.join(image_path,generate_log_filename(\"Figure_1\")).replace(\"gz\",\"png\")\n",
    "plt.savefig(filename, format = 'png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
